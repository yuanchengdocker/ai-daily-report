## AI洞察日报 2025/12/19

>  `AI 日报` 



### **今日摘要**

```
一项实验性项目训练了仅使用1913年前文本的时间锁定大型语言模型，可再现历史视角但包含时代偏见。通过Thunderbolt 5 RDMA技术可将多台Mac Studio的显存聚合，形成统一内存池以运行大型模型。社区讨论放任与受控两种LLM编程模式的利弊，以及T5Gemma 2不发布微调检查点引发的争议。
```



### **今日AI资讯**

1. ### **"时间锁定”LLM再现历史视角与偏见**
   一项实验性项目训练了仅使用1913年前文本的**大型语言模型**，如Ranke-4B-1913。这种**时间锁定模型**能够反映特定历史时期的知识、措辞和道德判断，例如对一战等后续事件一无所知，可用于历史研究和创作。然而，它也忠实地再现了旧语料中存在的**性别、种族和帝国主义等时代偏见**，引发了关于历史真实性与现代伦理责任的讨论。
   [原文链接](https://newshacker.me/story?id=46319826)

2. ### **Mac Studio通过Thunderbolt 5 RDMA扩展显存**
   一项技术实践展示了如何通过**RDMA over Thunderbolt 5**将多台Mac Studio的显存和系统内存聚合，形成约1.5TB的**统一内存池**。这种低延迟互联使得在消费级硬件上实现**张量并行**等策略变得可行，有助于本地运行大型模型。然而，实测数据显示，由于网络互连带宽和延迟的限制，多节点扩展的性能提升并非线性，存在瓶颈。
   [原文链接](https://newshacker.me/story?id=46319657)

3. ### **放任与受控：两种LLM编程模式的利弊**
   社区将利用**大型语言模型**编码的实践分为两类：几乎完全放任模型生成，或让模型在外部测试的反复约束下运行。完全放任的模式可能导致开发者陷入为模型做质量检查的耗费性工作，产生心理倦怠和资源浪费。而将模型用于快速原型构建和创意触发则被认为具有价值，但需要更好的提示工程和流程管理。
   [原文链接](https://newshacker.me/story?id=46318852)

4. ### **T5Gemma 2发布，检查点策略引争议**
   Google发布了下一代**编码器-解码器**模型T5Gemma 2，该模型在理解和生成任务上有所改进。然而，官方声明不会发布**后训练或指令微调检查点**，引发了社区不满。用户认为这阻碍了离线微调、定制化部署以及对小模型的实际应用。讨论还比较了**编码器-解码器**与**仅解码器**架构在推理成本、微调适用性和任务表现上的差异。
   [原文链接](https://newshacker.me/story?id=46317657)